# -*- coding: utf-8 -*-
"""taller en clase.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1289F1VjXUv-YuNcvOU4LOePsxPVMptzG
"""

# @title Preparación de entorno y GPU
import os, sys, torch, platform, subprocess, textwrap

print("Python:", sys.version)
print("PyTorch:", torch.__version__)
print("CUDA disponible:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("GPU:", torch.cuda.get_device_name(0))

os.makedirs("modelos", exist_ok=True)
os.makedirs("data", exist_ok=True)

# @title Script: train_transfer (versión con 1000 datos)
import os, time, random, csv
from pathlib import Path
from collections import defaultdict

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split, Subset

import torchvision
from torchvision import datasets, transforms, models

from sklearn.metrics import accuracy_score, f1_score


def set_seed(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = False
    torch.backends.cudnn.benchmark = True


def ensure_dir(path: str):
    Path(path).mkdir(parents=True, exist_ok=True)


def get_device():
    return torch.device("cuda" if torch.cuda.is_available() else "cpu")


def imagenet_transforms(img_size=224):
    return transforms.Compose([
        transforms.Resize((img_size, img_size)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                             std=[0.229, 0.224, 0.225]),
    ])


def split_train_val(dataset, val_ratio=0.1):
    n_total = len(dataset)
    n_val = int(n_total * val_ratio)
    n_train = n_total - n_val
    return random_split(dataset, [n_train, n_val],
                        generator=torch.Generator().manual_seed(42))


def replace_last_layer_vgg16(model: models.VGG, num_classes: int):
    in_features = model.classifier[6].in_features
    model.classifier[6] = nn.Linear(in_features, num_classes)
    return model


def replace_last_layer_resnet50(model: models.ResNet, num_classes: int):
    in_features = model.fc.in_features
    model.fc = nn.Linear(in_features, num_classes)
    return model


def freeze_all_but_last(model, last_module_names=("classifier.6", "fc")):
    for name, param in model.named_parameters():
        param.requires_grad = False
    for name, param in model.named_parameters():
        if any(name.startswith(mn) or name == mn for mn in last_module_names):
            param.requires_grad = True


def train_one_epoch(model, loader, criterion, optimizer, device):
    model.train()
    running_loss = 0.0
    all_preds, all_labels = [], []
    for images, labels in loader:
        images, labels = images.to(device), labels.to(device)
        optimizer.zero_grad(set_to_none=True)
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item() * images.size(0)
        preds = outputs.argmax(dim=1)
        all_preds.append(preds.detach().cpu())
        all_labels.append(labels.detach().cpu())

    all_preds = torch.cat(all_preds).numpy()
    all_labels = torch.cat(all_labels).numpy()
    acc = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average="macro")
    epoch_loss = running_loss / len(loader.dataset)
    return epoch_loss, acc, f1


@torch.inference_mode()
def evaluate(model, loader, criterion, device):
    model.eval()
    running_loss = 0.0
    all_preds, all_labels = [], []
    for images, labels in loader:
        images, labels = images.to(device), labels.to(device)
        outputs = model(images)
        loss = criterion(outputs, labels)
        running_loss += loss.item() * images.size(0)
        preds = outputs.argmax(dim=1)
        all_preds.append(preds.detach().cpu())
        all_labels.append(labels.detach().cpu())

    all_preds = torch.cat(all_preds).numpy()
    all_labels = torch.cat(all_labels).numpy()
    acc = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average="macro")
    epoch_loss = running_loss / len(loader.dataset)
    return epoch_loss, acc, f1


def log_metrics_csv(csv_path, header, rows):
    write_header = not os.path.exists(csv_path)
    with open(csv_path, "a", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        if write_header:
            writer.writerow(header)
        for r in rows:
            writer.writerow(r)


def run_training(
    model_name: str,
    dataset_name: str,
    num_classes: int,
    save_path: str,
    data_root: str = "./data",
    batch_size: int = 64,
    epochs: int = 5,
    lr: float = 1e-3,
    weight_decay: float = 0.0,
    num_workers: int = 2,
    val_ratio: float = 0.1,
    seed: int = 42,
    img_size: int = 224,
):
    set_seed(seed)
    ensure_dir(os.path.dirname(save_path))
    ensure_dir(data_root)
    device = get_device()
    print(f"[{model_name}/{dataset_name}] device: {device}")

    tfm = imagenet_transforms(img_size=img_size)

    # === Dataset limitado a 1000 imágenes ===
    if dataset_name.lower() == "cifar10":
        full_train = datasets.CIFAR10(root=data_root, train=True, download=True, transform=tfm)
        testset = datasets.CIFAR10(root=data_root, train=False, download=True, transform=tfm)
    elif dataset_name.lower() == "cifar100":
        full_train = datasets.CIFAR100(root=data_root, train=True, download=True, transform=tfm)
        testset = datasets.CIFAR100(root=data_root, train=False, download=True, transform=tfm)
    else:
        raise ValueError("dataset_name debe ser 'cifar10' o 'cifar100'.")

    subset_indices = list(range(1000))  # usar solo 1000 ejemplos
    trainset_small = Subset(full_train, subset_indices)
    print(f"Usando {len(trainset_small)} imágenes para entrenamiento total (train+val).")

    # Split 80/20
    train_size = int(0.8 * len(trainset_small))
    val_size = len(trainset_small) - train_size
    trainset, valset = random_split(trainset_small, [train_size, val_size])
    print(f"→ Train: {len(trainset)} | Val: {len(valset)} | Test: {len(testset)}")

    train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)
    val_loader = DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)
    test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)

    if model_name.lower() == "vgg16":
        model = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)
        model = replace_last_layer_vgg16(model, num_classes)
        last_names = ("classifier.6",)
    elif model_name.lower() == "resnet50":
        model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)
        model = replace_last_layer_resnet50(model, num_classes)
        last_names = ("fc",)
    else:
        raise ValueError("model_name debe ser 'vgg16' o 'resnet50'.")

    freeze_all_but_last(model, last_module_names=last_names)
    model = model.to(device)

    params_to_update = [p for p in model.parameters() if p.requires_grad]
    optimizer = optim.Adam(params_to_update, lr=lr, weight_decay=weight_decay)
    criterion = nn.CrossEntropyLoss()

    history = defaultdict(list)
    csv_file = save_path.replace(".pth", "_metrics.csv")
    header = ["epoch", "train_loss", "train_acc", "train_f1", "val_loss", "val_acc", "val_f1"]
    print(f"Métricas por época -> {csv_file}")

    best_val_acc = 0.0
    best_state = None

    for epoch in range(1, epochs + 1):
        t0 = time.time()
        train_loss, train_acc, train_f1 = train_one_epoch(model, train_loader, criterion, optimizer, device)
        val_loss, val_acc, val_f1 = evaluate(model, val_loader, criterion, device)
        dt = time.time() - t0

        history["train_loss"].append(train_loss)
        history["train_acc"].append(train_acc)
        history["train_f1"].append(train_f1)
        history["val_loss"].append(val_loss)
        history["val_acc"].append(val_acc)
        history["val_f1"].append(val_f1)

        log_metrics_csv(csv_file, header, [[epoch, train_loss, train_acc, train_f1, val_loss, val_acc, val_f1]])

        print(f"[{model_name}/{dataset_name}] Epoch {epoch:02d}/{epochs} "
              f"| train_acc={train_acc:.4f} f1={train_f1:.4f} "
              f"| val_acc={val_acc:.4f} f1={val_f1:.4f} "
              f"| {dt:.1f}s")

        if val_acc > best_val_acc:
            best_val_acc = val_acc
            best_state = {k: v.cpu() for k, v in model.state_dict().items()}

    if best_state is not None:
        model.load_state_dict(best_state)

    test_loss, test_acc, test_f1 = evaluate(model, test_loader, criterion, device)
    print(f"[{model_name}/{dataset_name}] TEST -> loss={test_loss:.4f} acc={test_acc:.4f} f1_macro={test_f1:.4f}")

    torch.save(model.state_dict(), save_path)
    print(f"Pesos guardados en: {save_path}")

    summary_path = save_path.replace(".pth", "_summary.txt")
    with open(summary_path, "w", encoding="utf-8") as f:
        f.write(f"Modelo: {model_name}\n")
        f.write(f"Dataset: {dataset_name}\n")
        f.write(f"Épocas: {epochs}\n")
        f.write(f"Batch size: {batch_size}\n")
        f.write(f"LR: {lr}\n")
        f.write(f"Train: {len(trainset)} | Val: {len(valset)} | Test: {len(testset)}\n")
        f.write(f"Val Acc (mejor): {best_val_acc:.4f}\n")
        f.write(f"Test Acc: {test_acc:.4f}\n")
        f.write(f"Test F1-macro: {test_f1:.4f}\n")
        f.write(f"Métricas por época: {csv_file}\n")

    return {
        "best_val_acc": best_val_acc,
        "test_acc": test_acc,
        "test_f1": test_f1,
        "metrics_csv": csv_file,
        "weights_path": save_path,
    }

# @title Entrenar VGG16 en CIFAR-10 (solo última capa)
out_vgg = run_training(
    model_name="vgg16",
    dataset_name="cifar10",
    num_classes=10,
    save_path="modelos/vgg16_cifar10.pth",
    data_root="./data",
    batch_size=64,
    epochs=5,          # ajusta si quieres
    lr=1e-3,
    weight_decay=0.0,
    num_workers=2,
    img_size=224,
    seed=42,
)
out_vgg

# @title Entrenar ResNet50 en CIFAR-100 (solo última capa)
out_res = run_training(
    model_name="resnet50",
    dataset_name="cifar100",
    num_classes=100,
    save_path="modelos/resnet50_cifar100.pth",
    data_root="./data",
    batch_size=64,
    epochs=5,          # ajusta si quieres
    lr=1e-3,
    weight_decay=0.0,
    num_workers=2,
    img_size=224,
    seed=42,
)
out_res

# @title Empaquetar y descargar modelos + métricas
import shutil
from google.colab import files

zip_path = "modelos.zip"
if os.path.exists(zip_path):
    os.remove(zip_path)

shutil.make_archive("modelos", "zip", "modelos")
files.download(zip_path)